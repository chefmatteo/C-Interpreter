# C-Interpreter

> Project: Rewrite the project `C4` with a easier to understand approach;
> [c4](https://github.com/rswier/c4), created by the author rswier;

The purpose of this project is to provide a clear and understandable rewrite of the original `c4` projectâ€”a minimal C compiler that became popular for its ability to self-host (compile itself) using only four core functions. While the original `c4` is impressive for its compactness and completeness, its code is intentionally obfuscated and difficult to follow, as it was designed to minimize code size above all else. This project aims to retain the core functionality and educational value of `c4`, but with clean, well-structured, and well-documented code that is accessible to learners and developers interested in compiler construction.

## There are mainly four aspects we gonna focus on in this project: 
#### How to Represent Syntax (BNF, etc.)

- Syntax of programming languages is often described using formal grammars.
- Backus-Naur Form (BNF) is a common notation for expressing context-free grammars.
- Extended BNF (EBNF) adds more expressive power and convenience.

#### Lexical Analysis: Finite and Infinite Automata

- Lexical analysis breaks source code into tokens (identifiers, keywords, symbols, etc.).
- Finite Automata (FA) are used to recognize regular languages, which are suitable for token patterns.
  - Deterministic Finite Automata (DFA) and Nondeterministic Finite Automata (NFA) are two types.
- Infinite automata are not typically used in lexical analysis; regular languages are always recognized by finite automata.

#### Syntax Analysis: Recursive Descent, LL(k), LALR Parsing

- Syntax analysis (parsing) checks if token sequences conform to the grammar.
- Recursive Descent Parsing is a top-down parsing technique, easy to implement for LL grammars.
- LL(k) parsers read input Left-to-right, produce a Leftmost derivation, and use k tokens of lookahead.
- LALR (Look-Ahead LR) parsers are a type of bottom-up parser, commonly used in parser generators (like Yacc/Bison), and can handle a wider range of grammars than LL parsers.
- èªæ³•åˆ†æï¼ˆparsingï¼‰å°±ä¿‚æª¢æŸ¥ä¸€ä¸² token ä¿‚å’ªè·Ÿ grammar è¦å‰‡ã€‚
- Recursive Descent Parsing ä¿‚ä¸€ç¨® top-down è§£ææ–¹æ³•ï¼Œå¥½æ˜“ implementï¼Œå•±ç”¨ LL grammarã€‚
- LL(k) parser ä¿‚ç”±å·¦è‡³å³è®€ inputï¼Œåšå·¦æ¨å°ï¼ˆLeftmost derivationï¼‰ï¼Œå¯ä»¥ç‡å‰é¢ k å€‹ token å¹«æ‰‹æ±ºå®šé» parseã€‚
- LALRï¼ˆLook-Ahead LRï¼‰parser ä¿‚ bottom-up è§£æï¼Œé€šå¸¸ç”¨ parser generatorï¼ˆå¥½ä¼¼ Yacc/Bisonï¼‰æ•´ï¼Œè™•ç† grammar ç¯„åœæ¯” LL parser æ›´åŠ å»£ã€‚

#### Intermediate Code Representation

- Intermediate code is an abstraction between source code and machine code.
- Common forms include three-address code, quadruples, and abstract syntax trees (AST).
- Intermediate code simplifies optimization and code generation.
- ä¸­é–“ç¢¼ï¼ˆIntermediate codeï¼‰ä¿‚ source code åŒæ©Ÿå™¨ç¢¼ä¹‹é–“å˜…ä¸€å±¤æŠ½è±¡ã€‚
- å¸¸è¦‹æœ‰ä¸‰åœ°å€ç¢¼ï¼ˆthree-address codeï¼‰ã€å››å…ƒå¼ï¼ˆquadruplesï¼‰ã€æŠ½è±¡èªæ³•æ¨¹ï¼ˆASTï¼‰ã€‚
- ç”¨ä¸­é–“ç¢¼å¯ä»¥æ–¹ä¾¿åšå„ªåŒ–åŒç”Ÿæˆæ©Ÿå™¨ç¢¼ã€‚

## Code Generation

- Code generation translates intermediate code into target machine code or assembly.
- Involves instruction selection, register allocation, and handling calling conventions.
- ä»£ç¢¼ç”Ÿæˆï¼ˆCode generationï¼‰å°±ä¿‚å°‡ä¸­é–“ç¢¼è½‰åšç›®æ¨™æ©Ÿå™¨ç¢¼æˆ–è€…çµ„åˆèªè¨€ã€‚
- åŒ…æ‹¬æŒ‡ä»¤é¸æ“‡ã€æš«å­˜å™¨åˆ†é…ã€åŒåŸ‹è™•ç†å‘¼å«è¦å‰‡ï¼ˆcalling conventionï¼‰ã€‚

## Code Optimization

- Code optimization improves the efficiency of the generated code.
- Can be performed at various stages: source-level, intermediate code, or machine code.
- Techniques include dead code elimination, constant folding, loop optimization, and inlining.
- ä»£ç¢¼å„ªåŒ–ï¼ˆCode optimizationï¼‰ä¿‚ä»¤ç”Ÿæˆå‡ºåšŸå˜…ä»£ç¢¼è·‘å¾—å¿«å•²æˆ–è€…æ…³è³‡æºã€‚
- å¯ä»¥å–ºå””åŒéšæ®µåšï¼šsource codeã€intermediate codeã€machine codeã€‚
- æŠ€å·§æœ‰ï¼šæ­»ç¢¼æ¶ˆé™¤ï¼ˆdead code eliminationï¼‰ã€å¸¸æ•¸æ‘ºç–Šï¼ˆconstant foldingï¼‰ã€è¿´åœˆå„ªåŒ–ï¼ˆloop optimizationï¼‰ã€inline å±•é–‹ï¼ˆinliningï¼‰ã€‚

## A question beforehand: Interpreter vs Compiler:

- **Interpreter:**
  - Executes source code directly, line by line or statement by statement.
  - No separate output file; runs the program immediately.
  - Easier to debug and test code interactively.
  - Slower execution speed compared to compiled code.
  - Examples: Python, Ruby, JavaScript.

- **Compiler:**
  - Translates source code into machine code or intermediate code before execution.
  - Produces an output file (e.g., executable or bytecode) that can be run later.
  - Program runs faster after compilation.
  - Errors are detected before execution, during the compilation step.
  - Examples: C, C++, Rust, Go.


In this project, when we say is a "C language compiler," what we're actually building is a C language interpreter. This means we can run C source code files directly, just like running a script. There are two main reasons for doing this:

1. The only real difference between an interpreter and a compiler is in the code generation stage; other parts like lexical analysis and parsing are the same.
2. Building an interpreter requires us to implement our own virtual machine and instruction set, which helps us better understand how computers work under the hood.

### Practial steps to construct a compiler: 
Generally, writing a compiler involves three main steps:

1. Lexical analysis(è©æ³•åˆ†æå™¨): converting the input string into an internal representation (tokens).
    - A **token** is a basic unit of meaning identified during lexical analysis. In the context of a compiler, tokens are categories such as keywords (e.g., `int`, `if`), identifiers (variable names), operators (`+`, `-`), literals (numbers, strings), and punctuation (semicolons, parentheses). The lexer scans the input source code and groups characters into these tokens, which are then used by the parser to understand the structure of the code.

2. Syntax analysis(èªæ³•åˆ†æå™¨): using the stream of tokens from lexical analysis to build a syntax tree.
3. Code generation: transforming the syntax tree into target code.

There are many tools available to help with steps 1 and 2, such as flex for lexical analysis and bison for syntax analysis. However, these tools are very powerful and hide many implementation details, which is not ideal for learning how to build a compiler from scratch. Therefore, we will implement these components by hand.

In this project, we will follow these steps to build our compiler:

1. Build our own virtual machine and instruction set. The target code we generate will be for this instruction set.
2. Implement our own lexical analyzer.
3. Implement our own syntax analyzer.

### Framework for the compiler:
### Compiler Framework

Our compiler mainly consists of four functions:

- `next()`: Performs lexical analysis and retrieves the next token. It automatically skips whitespace characters.
- `program()`: The entry point for syntax analysis, responsible for parsing the entire C program.
- `expression(level)`: Parses an expression. This function is separated out because expressions are relatively independent and complex in syntax analysis, so we modularize it as its own function.
- `eval()`: The entry point for the virtual machine, responsible for interpreting the generated target code.

For the hardcoded version of the framework, please refer to `framework.c`.
**Important**: 
- argc implies: number of arguments; 
- argv implies array of arguments; 
- The program will read a doucument of C langauge, and parse every character in the document, then provide output. 

> Expected format for argc and argv:
- argc: The number of command-line arguments passed to the program, including the program name itself as argv[0].
- argv: An array of character pointers (strings), where:
    - argv[0] is the name of the program (e.g., "./framework").
    - argv[1] to argv[argc-1] are the actual arguments provided by the user.

Example:
- If the program is run as: `./framework test.c`

```
    argc == 2
    argv[0] == "./framework"
    argv[1] == "test.c"
```

### Computer architecture and working principle: 
- In general, a computer has three main components we need to care about: the CPU, registers(å¯„å­˜å™¨), and memory.
> Registersï¼ˆå¯„å­˜å™¨ï¼‰ä¿‚ CPU å…¥é¢å˜…ä¸€å•²è¶…é«˜é€Ÿç´°ç´°ç²’å˜…è¨˜æ†¶é«”ï¼Œç”¨åšŸæš«å­˜æ•¸æ“šåŒæŒ‡ä»¤ã€‚ä¾‹å¦‚ä½ åšåŠ æ•¸ã€æ¸›æ•¸ã€æ¬é‹æ•¸æ“šï¼Œå…¨éƒ¨éƒ½æœƒç¶“é registersã€‚ä½¢å“‹å¥½ä¼¼ä½ åšé‹ç®—æ™‚æ‰‹ä¸Šæ‹ä½å˜…è¨ˆæ•¸æ©Ÿï¼Œå¿«éä½ å»è¨˜ä½å•²æ•¸å–ºæ™®é€šè¨˜æ†¶é«”ï¼ˆRAMï¼‰åº¦ã€‚æ¯å€‹ register éƒ½æœ‰å””åŒç”¨é€”ï¼Œä¾‹å¦‚å„²å­˜é‹ç®—çµæœã€è¨˜ä½ä¸‹ä¸€æ¢è¦åŸ·è¡Œå˜…æŒ‡ä»¤ä½ç½®ï¼ˆprogram counterï¼‰ã€æˆ–è€…æš«å­˜æŸå•²è®Šæ•¸ã€‚å› ç‚º registers å–º CPU å…¥é¢ï¼Œæ‰€ä»¥è®€å¯«é€Ÿåº¦æ¥µå¿«ï¼Œå°é›»è…¦é‹ä½œå¥½é‡è¦ã€‚

- The code (such as assembly instructions) is stored in memory as binary data. The CPU loads and executes these instructions one by one. The current state of the program (like variable values and where you are in the code) is kept in the registers.

**Memory**
- Memory is used to store data, which can be either your program's code or other information. 
- Modern operating systems use something called "virtual memory" instead of directly using the physical memory (RAM) in your computer.
  >  Windowsã€macOSã€Linuxï¼‰æœƒç”¨ä¸€ç¨®å«ã€Œè™›æ“¬è¨˜æ†¶é«”ã€å˜…æŠ€è¡“ï¼Œå””ä¿‚ç›´æ¥ç”¨ä½ éƒ¨é›»è…¦å˜…å¯¦é«”è¨˜æ†¶é«”ï¼ˆRAMï¼‰ã€‚
- Virtual memory acts like a map: it lets programs use a large range of addresses (for example, on a 32-bit system, 2^32 up to 4GB), even if the actual physical memory is much less (like 256MB).
- The operating system handles the mapping between virtual addresses and real memory.

**Program's memory is divided into several sections, called "segments."**
- **Text segment (code):** Stores the program's instructions.  
  - The actual instructions your CPU will execute (the binary code generated from your source code) are stored in the text segment of memory.

- **Data segment:** Stores variables that are initialized at the start.  
  *Example:* `int i = 10;` â€” the value 10 is stored in the data segment.

- **BSS segmentï¼ˆæœªåˆå§‹åŒ–æ•¸æ“šæ®µï¼‰:** Stores variables that are declared but not 
initialized.  
  *Example:* `int arr[1000];` Since we don't care about the initial values, these are put in the BSS segment to save space.

- **Stack:** Used for function calls, local variables, and keeping track of where to return after a function finishes.  
  *Example:* When you call a function, its local variables and return address are stored on the stack.

- **Heap:** Used for dynamic memory allocation (memory you request at runtime).  
  *Example:* When you use `malloc` in C or `new` in C++, the memory comes from the heap.

## Memory Layout Visualization

The following diagram shows the typical memory layout of a program in virtual memory:

```mermaid
graph TD
    %% Memory Layout Diagram
    subgraph "Virtual Memory Address Space"
        A["ğŸ“š Stack<br/>(High Address)<br/>Function calls, local variables<br/>Grows downward"]
        B["ğŸ—‚ï¸ Heap<br/>Dynamic memory allocation<br/>malloc(), free()<br/>Grows upward"]
        C["ğŸ“‹ BSS Segment<br/>Uninitialized global/static variables<br/>Zero-initialized"]
        D["ğŸ’¾ Data Segment<br/>Initialized global/static variables<br/>Constants, initialized data"]
        E["âš™ï¸ Text Segment<br/>(Low Address)<br/>Executable code<br/>Read-only"]
    end
    
    %% Address flow
    A -->|"High Address"| B
    B -->|"Growing upward"| C
    C -->|"Static data"| D
    D -->|"Low Address"| E

    
    class A stackStyle
    class B heapStyle
    class C bssStyle
    class D dataStyle
    class E textStyle
```



#### Future improvements and references:
- [Let's Build a Compiler](http://compilers.iecc.com/crenshaw/)
- [Lemon Parser Generator](http://www.hwaci.com/sw/lemon/)